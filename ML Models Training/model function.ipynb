{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasted list length: [1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 18, 19, 20, 20, 22, 23, 23, 25, 26, 26, 28, 29, 29, 31, 32, 32, 34, 34, 36, 37, 37, 39, 40, 41, 42, 43, 44, 45, 45, 46, 47, 49, 49, 51, 51, 52, 54, 55, 55, 57, 58, 58, 60, 61, 62, 62, 64, 65, 66, 67, 67, 69, 70, 70, 72, 73, 74, 75, 76, 77, 78, 79, 79, 80, 82, 83, 84, 85, 86, 87, 87, 88, 90, 91, 91, 93, 94, 94, 96, 96, 98, 98, 99, 101, 101, 103, 104, 104, 106, 107, 107, 109, 109, 110, 112, 113, 114, 115, 115, 117, 117, 118, 119, 121, 122, 122, 124, 125, 126, 127, 128, 128, 130, 131, 131, 133, 134, 134, 136, 136, 138, 139, 140, 140, 142, 143, 144, 145, 145, 147, 147, 149, 150, 151, 152, 153, 153, 155, 156, 156, 158, 158, 160, 160, 162, 162, 164, 165, 166, 167, 168, 169, 169, 171, 172, 173, 173, 175, 176, 177, 177, 178, 179, 181, 182, 183, 183, 185, 186, 187, 187, 188, 190, 191, 192, 193, 194, 195, 196, 196, 198, 199, 199, 201, 201, 203, 204, 204, 206, 207, 208, 208, 209, 210, 211, 212, 214, 215, 216, 217, 218, 219, 219, 221, 222, 223, 224, 225, 226, 226, 227, 228, 230, 231, 232, 233, 233, 235, 236, 236, 237, 239, 240, 240, 242, 243, 244, 244, 246, 247, 248, 248, 250, 251, 252, 253, 254, 254, 256, 257, 257, 259, 260, 261, 262, 262, 263, 264, 265, 266, 268, 268, 269, 271, 272, 273, 273, 275, 275, 276, 278, 278, 279, 281, 282, 282, 284, 285, 285, 287, 288, 288, 290, 291, 291, 292, 294, 294, 296, 297, 298, 299, 299, 301, 302, 303, 304, 305, 306, 307, 307, 308, 310, 310, 312, 312, 314, 314, 316, 316, 317, 318, 320, 321, 322, 323, 323, 325, 325, 327, 328, 329, 330, 331, 331, 332, 334, 335, 336, 337, 338, 338, 339, 340, 341, 343, 344, 345, 346, 347, 347, 348, 350, 350, 352, 352, 354, 355, 356, 357, 358, 358, 360, 361, 362, 363, 364, 365, 366, 366, 368, 368, 370, 370, 372, 373, 374, 375, 376, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 391, 392, 394, 395, 396, 397, 397, 399, 400, 401, 401, 402, 403, 404, 405, 406, 408, 409, 410, 411, 412, 412, 414, 414, 416, 417, 417, 419, 420, 420, 422, 423, 423, 425, 426, 426, 427, 429, 429, 430, 432, 433, 434, 435, 436, 436, 437, 439, 439, 441, 442, 443, 444, 445, 446, 447, 448, 449, 449, 450, 452, 453, 454, 455, 456, 457, 458, 458, 459, 461, 462, 463, 464, 464, 466, 467, 468, 469, 469, 470, 471, 472, 473, 474, 476, 476, 478, 479, 480, 481, 482, 482, 484, 485, 486, 487, 488, 489, 490, 491, 492, 492, 493, 495, 495, 497, 497, 499, 499, 501, 501, 503, 504, 505, 505, 506, 508, 509, 509, 511, 511, 513, 513, 515, 516, 517, 518, 518, 519, 521, 522, 523, 524, 525, 525, 527, 528, 529, 529, 531, 531, 532, 533, 535, 536, 537, 538, 539, 539, 541, 542, 542, 544, 545, 546, 546, 547, 548, 550, 551, 551, 553, 554, 554, 556, 557, 558, 559, 560, 560, 562, 563, 564, 564, 565, 566, 567, 569, 569, 571, 571, 573, 574, 574, 576, 577, 577, 579, 580, 580, 581, 582, 583, 584, 585, 587, 588, 589, 589, 591, 592, 593, 594, 594, 595, 597, 598, 599, 599, 601, 601, 602, 603, 604, 606, 607, 607, 609, 609, 611, 612, 612, 614, 615, 615, 617, 618, 618, 620, 620, 622, 623, 624, 625, 625, 626, 627, 628, 630, 630, 632, 632, 634, 634, 636, 637, 638, 638, 639, 641, 642, 643, 644, 644, 646, 646, 648, 648, 650, 651, 652, 652, 654, 655, 655, 656, 657, 659, 660, 661, 661, 663, 664, 665, 666, 667, 668, 668, 670, 670, 672, 672, 674, 675, 676, 677, 678, 678, 680, 681, 682, 682, 683, 685, 685, 687, 687, 689, 689, 690, 692, 692, 694, 695, 695, 697, 697, 699, 699, 701, 702, 703, 704, 705, 706, 706, 708, 709, 709, 711, 712, 712, 713, 715, 716, 716, 718, 719, 720, 721, 721, 723, 724, 724, 725, 727, 727, 729, 729, 731, 732, 733, 733, 735, 736, 736, 738, 738, 739, 740, 742, 742, 743, 744, 745, 747, 747, 749, 750, 750, 752, 752, 754, 754, 756, 757, 757, 759, 759, 761, 762, 763, 764, 765, 765, 767, 768, 769, 770, 770, 771, 772, 773, 775, 776, 776, 777, 779, 780, 780, 781, 783, 783, 785, 786, 787, 787, 788, 789, 790, 792, 792, 794, 794, 796, 796, 798, 798, 800, 800, 801, 803, 804, 804, 806, 807, 807, 809, 810, 811, 811, 813, 814, 815, 816, 816, 817, 819, 819, 821, 822, 823, 824, 824, 825, 826, 828, 829, 829, 831, 832, 832, 834, 835, 835, 837, 837, 838, 840, 840, 841, 842, 843, 845, 845, 847, 848, 849, 849, 850, 852, 853, 853, 854, 856, 857, 857, 859, 860, 861, 861, 863, 863, 865, 866, 867, 868, 868, 869, 871, 871, 873, 874, 875, 876, 877, 877, 879, 880, 881, 881, 883, 884, 885, 886, 886, 888, 888, 889, 891, 892, 893, 893, 895, 896, 896, 897, 899, 900, 901, 902, 903, 903, 904, 905, 907, 907, 909, 910, 910, 912, 912, 913, 915, 915, 917, 917, 919, 920, 920, 921, 923, 924, 925, 926, 926, 928, 928, 930, 931, 932, 933, 934, 935, 936, 937, 937, 939, 940, 941, 941, 942, 943, 945, 945, 946, 948, 949, 950, 951, 951, 952, 954, 955, 956, 956, 958, 959, 960, 961, 961, 962, 963, 965, 966, 967, 967, 968, 969, 971, 972, 972, 973, 975, 976, 977, 977, 978, 979, 981, 982, 983, 983, 985, 986, 986, 987, 989, 990, 990, 991, 993, 994, 995, 995, 997, 997, 999, 1000, 1001, 1001, 1003, 1004, 1005, 1006, 1007, 1007, 1009, 1010, 1010, 1011, 1012, 1014, 1014, 1015, 1017, 1018, 1019, 1019, 1020, 1021, 1022, 1023, 1025, 1025, 1026, 1028, 1028, 1030, 1031, 1032, 1032, 1033, 1034, 1036, 1037, 1038, 1038, 1039, 1040, 1042, 1043, 1043, 1044, 1046, 1046, 1048, 1049, 1050, 1050, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1059, 1061, 1061, 1063, 1064, 1065, 1066, 1066, 1067, 1069, 1070, 1071, 1072, 1073, 1073, 1075, 1075, 1077, 1077, 1079, 1079, 1081, 1082, 1082, 1084, 1085, 1086, 1086, 1087, 1089, 1090, 1091, 1091, 1092, 1094, 1095, 1096, 1096, 1098, 1099, 1100, 1100, 1102, 1102, 1103, 1104, 1106, 1107, 1108, 1108, 1109, 1110, 1111, 1112, 1113, 1115, 1116, 1117, 1118, 1119, 1119, 1121, 1122, 1123, 1123, 1124, 1125, 1126, 1128, 1128, 1129, 1130, 1131, 1132, 1134, 1134, 1135, 1137, 1138, 1138, 1139, 1141, 1141]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def linearRegressionNextElements(input_list):\n",
    "    # Convert input list to numpy array\n",
    "    X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Start forecasting until the size of the list reaches 1413\n",
    "    while len(input_list) < 1143:\n",
    "        # Fit the model\n",
    "        model.fit(X, input_list)\n",
    "        \n",
    "        # Forecast the next element\n",
    "        next_element = model.predict([[len(input_list)]])[0]  # Extract single element\n",
    "        \n",
    "        # Append the forecasted element to the list\n",
    "        input_list.append(int(next_element))\n",
    "        \n",
    "        # Update the input array with the new data\n",
    "        X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    return input_list\n",
    "\n",
    "# Example usage:\n",
    "input_list = [1, 2, 3, 4, 5]  # Provide your list of integers here\n",
    "forecasted_list = linearRegressionNextElements(input_list)\n",
    "print(\"Forecasted list length:\", (forecasted_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasted list length: [1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def logisticRegressionNextElements(input_list):\n",
    "    # Convert input list to numpy array\n",
    "    X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    # Start forecasting until the size of the list reaches 1413\n",
    "    while len(input_list) < 1413:\n",
    "        # Fit the model\n",
    "        model.fit(X, input_list)\n",
    "        \n",
    "        # Forecast the next element\n",
    "        next_element = model.predict([[len(input_list)]])[0]  # Extract single element\n",
    "        \n",
    "        # Append the forecasted element to the list\n",
    "        input_list.append(int(next_element))\n",
    "        \n",
    "        # Update the input array with the new data\n",
    "        X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    return input_list\n",
    "\n",
    "# Example usage:\n",
    "input_list = [1, 2, 3, 4, 5]  # Provide your list of integers here\n",
    "forecasted_list = logisticRegressionNextElements(input_list)\n",
    "print(\"Forecasted list length:\", (forecasted_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasted list length: [1, 2, 3, 4, 5, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def svmNextElements(input_list):\n",
    "    # Convert input list to numpy array\n",
    "    X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    model = SVR()\n",
    "    \n",
    "    # Start forecasting until the size of the list reaches 1413\n",
    "    while len(input_list) < 1413:\n",
    "        # Fit the model\n",
    "        model.fit(X, input_list)\n",
    "        \n",
    "        # Forecast the next element\n",
    "        next_element = model.predict([[len(input_list)]])[0]  # Extract single element\n",
    "        \n",
    "        # Append the forecasted element to the list\n",
    "        input_list.append(int(next_element))\n",
    "        \n",
    "        # Update the input array with the new data\n",
    "        X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    return input_list\n",
    "\n",
    "# Example usage:\n",
    "input_list = [1, 2, 3, 4, 5]  # Provide your list of integers here\n",
    "forecasted_list = svmNextElements(input_list)\n",
    "print(\"Forecasted list length:\", (forecasted_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasted list length: [1, 2, 3, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def randomForestNextElements(input_list):\n",
    "    # Convert input list to numpy array\n",
    "    X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    model = RandomForestRegressor()\n",
    "    \n",
    "    # Start forecasting until the size of the list reaches 1413\n",
    "    while len(input_list) < 1413:\n",
    "        # Fit the model\n",
    "        model.fit(X, input_list)\n",
    "        \n",
    "        # Forecast the next element\n",
    "        next_element = model.predict([[len(input_list)]])[0]  # Extract single element\n",
    "        \n",
    "        # Append the forecasted element to the list\n",
    "        input_list.append(int(next_element))\n",
    "        \n",
    "        # Update the input array with the new data\n",
    "        X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    return input_list\n",
    "\n",
    "# Example usage:\n",
    "input_list = [1, 2, 3, 4, 5]  # Provide your list of integers here\n",
    "forecasted_list = randomForestNextElements(input_list)\n",
    "print(\"Forecasted list length:\", (forecasted_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def mlpNextElements(input_list):\n",
    "    # Convert input list to numpy array\n",
    "    X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    model = MLPRegressor()\n",
    "    \n",
    "    # Start forecasting until the size of the list reaches 1413\n",
    "    while len(input_list) < 1413:\n",
    "        # Fit the model\n",
    "        model.fit(X, input_list)\n",
    "        \n",
    "        # Forecast the next element\n",
    "        next_element = model.predict([[len(input_list)]])[0]  # Extract single element\n",
    "        \n",
    "        # Append the forecasted element to the list\n",
    "        input_list.append(int(next_element))\n",
    "        \n",
    "        # Update the input array with the new data\n",
    "        X = np.array(input_list).reshape(-1, 1)\n",
    "    \n",
    "    return input_list\n",
    "\n",
    "# Example usage:\n",
    "input_list = [1, 2, 3, 4, 5]  # Provide your list of integers here\n",
    "forecasted_list = mlpNextElements(input_list)\n",
    "print(\"Forecasted list length:\", (forecasted_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "def forecast_list(lst):\n",
    "    # Convert the list to numpy array\n",
    "    data = np.array(lst)\n",
    "    \n",
    "    # Reshape the data for LSTM input (samples, time steps, features)\n",
    "    data = data.reshape((1, len(lst), 1))\n",
    "    \n",
    "    # Define LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(len(lst), 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(data, data, epochs=200, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Forecast next elements until the list reaches size 1413\n",
    "    while len(lst) < 1143:\n",
    "        # Reshape data for forecasting\n",
    "        input_data = np.array(lst[-len(lst):]).reshape((1, len(lst), 1))\n",
    "        # Predict next element\n",
    "        next_element = model.predict(input_data, verbose=0)[0][0]\n",
    "        # Append predicted element to the list\n",
    "        lst.append(int(round(next_element)))\n",
    "    \n",
    "    return lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22/01/20</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23/01/20</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24/01/20</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25/01/20</td>\n",
       "      <td>1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26/01/20</td>\n",
       "      <td>2120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  Cases\n",
       "0  22/01/20    557\n",
       "1  23/01/20    657\n",
       "2  24/01/20    944\n",
       "3  25/01/20   1437\n",
       "4  26/01/20   2120"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Dataset/global_cases.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[557, 657, 944, 1437, 2120, 2929, 5580, 6169, 8237, 9927, 12038, 16787, 19887, 23899, 27644, 30806, 34400, 37131, 40162, 42771, 44814, 45232, 60384, 66912, 69055, 71238, 73273, 75155, 75655, 76216, 76846, 78608, 78990, 79558, 80412, 81384, 82728, 84152, 86023, 88402, 90382, 92994, 95338, 98078, 102062, 106199, 109997, 114292, 119051, 126527, 133283, 146477, 157365, 168598, 183165, 198339, 215900, 242987, 272517, 304944, 339156, 381711, 423594, 475075, 535889, 599820, 669402, 725918, 790929, 869371, 955728, 1038176, 1122386, 1182507, 1254222, 1328948, 1397886, 1480062, 1567243, 1653665, 1729180, 1847761, 1919593, 2004226, 2082482, 2176950, 2264830, 2343082, 2419404, 2495886, 2571973, 2654011, 2736728, 2820291, 2903496, 2974291, 3045275, 3121107, 3198435, 3282087, 3371033, 3449295, 3523292, 3600929, 3680614, 3771036, 3860174, 3950657, 4035000, 4110223, 4186364, 4271644, 4355992, 4451634, 4547536, 4641426, 4719789, 4808921, 4904739, 5010162, 5116135, 5223477, 5327627, 5420843, 5509066, 5601428, 5703984, 5823977, 5945413, 6080456, 6183316, 6283580, 6406221, 6517437, 6655667, 6789073, 6918403, 7029415, 7132463, 7258445, 7395607, 7531793, 7659267, 7793175, 7925122, 8050034, 8193933, 8337135, 8480229, 8661225, 8847108, 8940170, 9082502, 9250625, 9422061, 9601576, 9794066, 9970478, 10138935, 10291617, 10475388, 10690516, 10895486, 11098124, 11287838, 11474058, 11642331, 11851900, 12067186, 12291782, 12524663, 12737483, 12930415, 13124893, 13345920, 13575706, 13820618, 14058038, 14293379, 14505270, 14713414, 14956435, 15231791, 15511422, 15794928, 16044485, 16256294, 16490680, 16754567, 17024801, 17306582, 17594044, 17840315, 18070926, 18276983, 18542287, 18817443, 19104145, 19386579, 19651806, 19884801, 20118013, 20383576, 20658646, 20948740, 21255813, 21504839, 21714880, 21927762, 22187166, 22464640, 22736119, 22998072, 23261601, 23464360, 23692460, 23937668, 24219359, 24506622, 24790561, 25051295, 25271033, 25535340, 25802224, 26084114, 26369544, 26671276, 26944997, 27173995, 27391278, 27637790, 27923100, 28227493, 28543855, 28832580, 29081415, 29348555, 29629107, 29934059, 30249632, 30576278, 30869397, 31124526, 31384435, 31668754, 31980407, 32301360, 32631974, 32918085, 33165079, 33426328, 33709128, 34033675, 34352787, 34682292, 34979647, 35240179, 35551524, 35867647, 36216841, 36576162, 36936565, 37292089, 37581583, 37871608, 38190980, 38571764, 38978760, 39389280, 39761413, 40092656, 40467606, 40856480, 41296091, 41778168, 42272069, 42728414, 43087247, 43570291, 44038754, 44560523, 45111898, 45683640, 46155745, 46594934, 47143303, 47708817, 48200961, 48820295, 49443478, 50042669, 50533518, 51043324, 51614069, 52238734, 52878929, 53539863, 54126889, 54617117, 55142751, 55745063, 56367328, 57021314, 57695345, 58287916, 58785996, 59320006, 59910792, 60527871, 61117132, 61812474, 62420041, 62915662, 63422239, 64037142, 64677389, 65367986, 66064345, 66706697, 67250638, 67785484, 68416150, 69081500, 70576907, 71282658, 71931348, 72470142, 73021298, 73669919, 74387461, 75126063, 75847501, 76483006, 77030307, 77576529, 78227986, 78910081, 79603472, 80096696, 80629340, 81042534, 81541719, 82213594, 82938894, 83778622, 84342314, 84946690, 85467020, 86024461, 86765351, 87556264, 88426222, 89261637, 90017904, 90619443, 91233917, 91926453, 92669726, 93428784, 94212637, 94869509, 95397119, 95914366, 96507148, 97194354, 97863110, 98526596, 99103312, 99565294, 100046846, 100608158, 101209122, 101824237, 102413219, 102928083, 103324692, 103767694, 104233148, 104758397, 105230171, 105766676, 106201874, 106553422, 106893339, 107300629, 107741093, 108184860, 108614449, 108994937, 109303582, 109584404, 109944197, 110330576, 110741376, 111151038, 111526248, 111845811, 112141380, 112534122, 112979052, 113432016, 113873956, 114270693, 114578478, 114883979, 115197242, 115639088, 116097496, 116546593, 116960620, 117335064, 117640533, 118054577, 118513444, 119003124, 119492372, 119947968, 120313043, 120667127, 121134161, 121679877, 122227806, 122790345, 123288792, 123729261, 124150452, 124656291, 125285990, 125938183, 126578225, 127162758, 127655412, 128114688, 128676894, 129350790, 130060609, 130705394, 131245744, 131805666, 132302655, 132910149, 133573288, 134409344, 135156662, 135827306, 136536918, 137150764, 137914588, 138725902, 139547133, 140406983, 141200970, 141901080, 142592012, 143431609, 144320851, 145220252, 146127911, 146952141, 147680627, 148369677, 149208799, 150107590, 151001625, 151892994, 152687904, 153375175, 154065632, 154850588, 155692589, 156565435, 157400261, 158188559, 158837968, 159463612, 160189411, 160956898, 161691597, 162401162, 163032774, 163579396, 164122938, 164742576, 165414621, 165696927, 166323402, 166895763, 167377386, 167828464, 168361871, 168930299, 169477196, 169982707, 170463127, 170853074, 171240130, 171700134, 172187762, 172668261, 173093152, 173489803, 173812784, 174134551, 174504668, 174922960, 175376464, 175798501, 176166881, 176469682, 176778984, 177153741, 177542916, 177938911, 178342418, 178689716, 178994480, 179291135, 179701349, 180097972, 180507231, 180924899, 181287796, 181599032, 181932050, 182313258, 182712726, 183148268, 183590610, 183964437, 184292181, 184670098, 185120629, 185585871, 186067750, 186580986, 187002099, 187371991, 187806155, 188331814, 188869796, 189446223, 190042161, 190531011, 190951220, 191440687, 191973106, 192529214, 193094403, 193822543, 194254462, 194700076, 195240703, 195847033, 196497116, 197139985, 197872339, 198389684, 198872752, 199443855, 200078130, 200758588, 201444210, 202263736, 202825005, 203292722, 203911805, 204557617, 205288310, 205994052, 206795847, 207348758, 207815449, 208479793, 209162480, 209898123, 210609857, 211395070, 211954124, 212402204, 213086398, 213763171, 214500582, 215237031, 215988154, 216545334, 216992164, 217664015, 218284677, 219003757, 219680532, 220404105, 220900816, 221347788, 221825561, 222482051, 223119762, 223759907, 224418618, 224887140, 225258543, 225836445, 226386274, 226953125, 227540479, 228163698, 228687569, 229050966, 229581157, 230069837, 230604815, 231152445, 231701691, 232075693, 232431512, 232914281, 233365258, 233856235, 234346132, 234868134, 235218776, 235527819, 235974883, 236397732, 236904785, 237366619, 237842039, 238182243, 238498696, 238886684, 239324892, 239781314, 240234710, 240692696, 241034054, 241347355, 241775141, 242206709, 242683018, 243145068, 243637892, 244008150, 244317353, 244769753, 245196224, 245710221, 246193247, 246687330, 247061943, 247410790, 247868435, 248272884, 248797676, 249315133, 249839415, 250245164, 250585863, 251110911, 251580325, 252153854, 252674737, 253274490, 253670214, 254018493, 254620079, 255126904, 255742113, 256363770, 256978407, 257416423, 257796215, 258500870, 259093463, 259751368, 260348589, 260962280, 261381085, 261770875, 262513214, 263113681, 263816145, 264523382, 265254220, 265726430, 266146231, 266849953, 267489200, 268200544, 268898928, 269592462, 270059366, 270485916, 271179799, 271816596, 272556770, 273300997, 274068563, 274598505, 275083461, 275893527, 276652188, 277566671, 278578699, 279434799, 280116791, 280692270, 282033889, 283370260, 285075127, 287025253, 288728291, 289931428, 290849939, 293188320, 295801747, 298385756, 301054978, 304008616, 306145200, 308150282, 311401997, 314432404, 317879058, 321066163, 324382616, 326810379, 328981578, 331759391, 335523995, 339607276, 343341570, 347151687, 349801473, 352184811, 355817671, 359471485, 363233158, 366932890, 370611819, 373175361, 375352380, 379211855, 382341587, 385526232, 388720309, 391675874, 393779507, 395542316, 398516432, 401116122, 403549117, 406363391, 408788228, 410513339, 411919279, 413962163, 415832953, 418072759, 420135672, 422119106, 423596187, 424789720, 426495378, 428169102, 430086590, 431874501, 433528807, 434802795, 435854949, 437463827, 439062564, 440722439, 442497646, 444260070, 445550401, 446660147, 448401620, 449985919, 451976099, 453847080, 455719680, 457280387, 458418032, 460415942, 462225344, 464464205, 466542109, 468433636, 469913015, 470873843, 472753728, 474730951, 476534854, 478319055, 480078626, 481203035, 482053796, 483898093, 485649915, 487247590, 488810861, 490229883, 491102544, 491795689, 493029039, 494394436, 495614407, 496892754, 498067151, 498809406, 499371019, 500357346, 501476932, 502551958, 503514561, 504173546, 504728004, 505135766, 505593027, 506611148, 507572864, 508494564, 509326073, 509804848, 510170201, 510807207, 511586152, 512270545, 513029820, 513674263, 514028003, 514337424, 514784854, 515456909, 516084404, 516684038, 517314925, 517654258, 517933111, 518623538, 519264799, 519959508, 520612153, 521185114, 521623840, 521997492, 522587922, 523196288, 523956899, 524552967, 525179079, 525546242, 525862056, 526355204, 526949796, 527593755, 528096493, 528666149, 528995178, 529271873, 529616224, 530241569, 530939976, 531470716, 532014270, 532267351, 532530338, 532965247, 533576907, 534283486, 534848531, 535405294, 535689085, 535887333, 536415150, 537272203, 537994296, 538577138, 539110151, 539338742, 539607241, 540362445, 541178509, 541984328, 542767959, 543545293, 543862216, 544148980, 545021364, 545963319, 546896134, 547974317, 548925491, 549284928, 549643412, 550495502, 551700794, 552926114, 553979237, 554985885, 555489725, 555912593, 556965871, 558188894, 559615969, 560618337, 561620875, 562173878, 562633320, 563831363, 565376142, 566978238, 568132310, 569227609, 569864429, 570415520, 571389219, 572585879, 574012504, 575128637, 576504042, 577080293, 577625050, 578498268, 579688847, 580844327, 582298507, 583199840, 584378161, 584876085, 585692322, 586732558, 587759595, 588765069, 589518048, 590154612, 590615366, 591470649, 592553956, 593544473, 594489234, 595290702, 595865130, 596349664, 597060819, 597949972, 598834543, 599699548, 600401903, 600854582, 601260827, 601837965, 602585861, 603339040, 604018804, 604676287, 605052969, 605423358, 605881148, 606554997, 607171546, 607755506, 608281134, 608594366, 608908659, 609393741, 609942218, 610640243, 611251541, 611707437, 612002534, 612257111, 612673963, 613227572, 613745750, 614365390, 614883451, 615148616, 615371539, 615794219, 616393758, 616943716, 617587849, 618060372, 618301429, 618502138, 618822004, 619422193, 620005197, 620645800, 621221027, 621440044, 621658996, 622112629, 622802836, 623353219, 624016113, 624494008, 624712097, 624904267, 625380762, 626009472, 626536431, 627131819, 627563033, 627745374, 627906174, 628292158, 628887107, 629354131, 629825004, 630188852, 630387961, 630555274, 630855587, 631257223, 631605553, 632099160, 632543650, 632744972, 632931615, 633236212, 633687028, 634121053, 634581453, 635069827, 635312334, 635495843, 635842682, 636346431, 636832747, 637312096, 637942820, 638190175, 638412606, 638754188, 639328415, 639919189, 640339580, 641037771, 641308397, 641571858, 641977647, 642708342, 643244123, 643950364, 644711955, 645011864, 645269011, 645774500, 646399010, 647032502, 647841604, 648512056, 648786733, 649085967, 649658720, 650436793, 651122541, 651873247, 652605304, 652905777, 653189501, 653775912, 654484792, 655205564, 655963530, 656604975, 656949354, 657135648, 657655053, 658309208, 658946340, 659617625, 660199728, 660463792, 660778387, 661198108, 661660893, 662231345, 662966595, 663685449, 664042494, 664369944, 664718936, 665113877, 665620426, 666097539, 666533383, 666754735, 666951599, 667166217, 667518310, 667896028, 668220526, 668562246, 668715650, 668830077, 669001131, 669272951, 669567171, 669848920, 670086844, 670220207, 670344054, 670503932, 670734589, 671032404, 671291489, 671529548, 671632287, 671721476, 671867463, 672065240, 672295338, 672554241, 672696324, 672828531, 672906177, 673044131, 673237731, 673477639, 673685532, 673878833, 673969796, 674056229, 674143589, 674323721, 674569824, 674790916, 674933342, 674978793, 675044414, 675171439, 675322238, 675542852, 675731911, 675914580, 675968775, 676024901, 676082941, 676213378, 676392824, 676570149]\n"
     ]
    }
   ],
   "source": [
    "cases_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    cases_list.append(row['Cases'])\n",
    "\n",
    "print(cases_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "foreCast = forecast_list(cases_list[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreCast = linearRegressionNextElements(cases_list[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[557, 657, 944, 1437, 2120, 2929, 5580, 6169, 8237, 9927, 12038, 16787, 19887, 23899, 27644, 30806, 34400, 37131, 40162, 42771, 44814, 45232, 60384, 66912, 69055, 71238, 73273, 75155, 75655, 76216, 76846, 78608, 78990, 79558, 80412, 81384, 82728, 84152, 86023, 88402, 90382, 92994, 95338, 98078, 102062, 106199, 109997, 114292, 119051, 126527, 133283, 146477, 157365, 168598, 183165, 198339, 215900, 242987, 272517, 304944, 339156, 381711, 423594, 475075, 535889, 599820, 669402, 725918, 790929, 869371, 955728, 1038176, 1122386, 1182507, 1254222, 1328948, 1397886, 1480062, 1567243, 1653665, 1729180, 1847761, 1919593, 2004226, 2082482, 2176950, 2264830, 2343082, 2419404, 2495886, 2571973, 2654011, 2736728, 2820291, 2903496, 2974291, 3045275, 3121107, 3198435, 3282087, 3371033, 3449295, 3523292, 3600929, 3680614, 3771036, 3860174, 3950657, 4035000, 4110223, 4186364, 4271644, 4355992, 4451634, 4547536, 4641426, 4719789, 4808921, 4904739, 5010162, 5116135, 5223477, 5327627, 5420843, 5509066, 5601428, 5703984, 5823977, 5945413, 6080456, 6183316, 6283580, 6406221, 6517437, 6655667, 6789073, 6918403, 7029415, 7132463, 7258445, 7395607, 7531793, 7659267, 7793175, 7925122, 8050034, 8193933, 8337135, 8480229, 8661225, 8847108, 8940170, 9082502, 9250625, 9422061, 9601576, 9794066, 9970478, 10138935, 10291617, 10475388, 10690516, 10895486, 11098124, 11287838, 11474058, 11642331, 11851900, 12067186, 12291782, 12524663, 12737483, 12930415, 13124893, 13345920, 13575706, 13820618, 14058038, 14293379, 14505270, 14713414, 14956435, 15231791, 15511422, 15794928, 16044485, 16256294, 16490680, 16754567, 17024801, 17306582, 17594044, 17840315, 18070926, 18276983, 18542287, 18817443, 19104145, 19386579, 19651806, 19884801, 20118013, 20383576, 20658646, 20948740, 21255813, 21504839, 21714880, 21927762, 22187166, 22464640, 22736119, 22998072, 23261601, 23464360, 23692460, 23937668, 24219359, 24506622, 24790561, 25051295, 25271033, 25535340, 25802224, 26084114, 26369544, 26671276, 26944997, 27173995, 27391278, 27637790, 27923100, 28227493, 28543855, 28832580, 29081415, 29348555, 29629107, 29934059, 30249632, 30576278, 30869397, 31124526, 31384435, 31668754, 31980407, 32301360, 32631974, 32918085, 33165079, 33426328, 33709128, 34033675, 34352787, 34682292, 34979647, 35240179, 35551524, 35867647, 36216841, 36576162, 36936565, 37292089, 37581583, 37871608, 38190980, 38571764, 38978760, 39389280, 39761413, 40092656, 40467606, 40856480, 41296091, 41778168, 42272069, 42728414, 43087247, 43570291, 44038754, 44560523, 45111898, 45683640, 46155745, 46594934, 47143303, 47708817, 48200961, 48820295, 49443478, 50042669, 50533518, 51043324, 51614069, 52238734, 52878929, 53539863, 54126889, 54617117, 55142751, 55745063, 56367328, 57021314, 57695345, 58287916, 58785996, 59320006, 59910792, 60527871, 61117132, 61812474, 62420041, 62915662, 63422239, 64037142, 64677389, 65367986, 66064345, 66706697, 67250638, 67785484, 68416150, 69081500, 70576907, 71282658, 71931348, 72470142, 73021298, 73669919, 74387461, 75126063, 75847501, 76483006, 77030307, 77576529, 78227986, 78910081, 79603472, 80096696, 80629340, 81042534, 81541719, 82213594, 82938894, 83778622, 84342314, 84946690, 85467020, 86024461, 86765351, 87556264, 88426222, 89261637, 90017904, 90619443, 91233917, 91926453, 92669726, 93428784, 94212637, 94869509, 95397119, 95914366, 96507148, 97194354, 97863110, 98526596, 99103312, 99565294, 100046846, 100608158, 101209122, 101824237, 102413219, 102928083, 103324692, 103767694, 104233148, 104758397, 105230171, 105766676, 106201874, 106553422, 106893339, 107300629, 107741093, 108184860, 108614449, 108994937, 109303582, 109584404, 109944197, 110330576, 110741376, 111151038, 111526248, 111845811, 112141380, 112534122, 112979052, 113432016, 113873956, 114270693, 114578478, 114883979, 115197242, 115639088, 116097496, 116546593, 116960620, 117335064, 117640533, 118054577, 118513444, 119003124, 119492372, 119947968, 120313043, 120667127, 121134161, 121679877, 122227806, 122790345, 123288792, 123729261, 124150452, 124656291, 125285990, 125938183, 126578225, 127162758, 127655412, 128114688, 128676894, 129350790, 130060609, 130705394, 131245744, 131805666, 132302655, 132910149, 133573288, 134409344, 135156662, 135827306, 136536918, 137150764, 137914588, 138725902, 139547133, 140406983, 141200970, 141901080, 142592012, 143431609, 144320851, 145220252, 146127911, 146952141, 147680627, 148369677, 149208799, 150107590, 151001625, 151892994, 152687904, 153375175, 154065632, 154850588, 155692589, 156565435, 157400261, 158188559, 158837968, 159463612, 160189411, 160956898, 161691597, 162401162, 163032774, 163579396, 164122938, 164742576, 165414621, 165696927, 166323402, 166895763, 167377386, 167828464, 168361871, 168930299, 169477196, 169982707, 170463127, 170853074, 171240130, 171700134, 172187762, 172668261, 173093152, 173489803, 173812784, 174134551, 174504668, 174922960, 175376464, 175798501, 176166881, 176469682, 176778984, 177153741, 177542916, 177938911, 178342418, 178689716, 178994480, 179291135, 179701349, 180097972, 180507231, 180924899, 181287796, 181599032, 181932050, 182313258, 182712726, 183148268, 183590610, 183964437, 184292181, 184670098, 185120629, 185585871, 186067750, 186580986, 187002099, 187371991, 187806155, 188331814, 188869796, 189446223, 190042161, 190531011, 190951220, 191440687, 191973106, 192529214, 193094403, 193822543, 194254462, 194700076, 195240703, 195847033, 196497116, 197139985, 197872339, 198389684, 198872752, 199443855, 200078130, 200758588, 201444210, 202263736, 202825005, 203292722, 203911805, 204557617, 205288310, 205994052, 206795847, 207348758, 207815449, 208479793, 209162480, 209898123, 210609857, 211395070, 211954124, 212402204, 213086398, 213763171, 214500582, 215237031, 215988154, 216545334, 216992164, 217664015, 218284677, 219003757, 219680532, 220404105, 220900816, 221347788, 221825561, 222482051, 223119762, 223759907, 224418618, 224887140, 225258543, 225836445, 226386274, 226953125, 227540479, 228163698, 228687569, 229050966, 229581157, 230069837, 230604815, 231152445, 231701691, 232075693, 232431512, 232914281, 233365258, 233856235, 234346132, 234868134, 235218776, 235527819, 235974883, 236397732, 236904785, 237366619, 237842039, 238182243, 238498696, 238886684, 239324892, 239781314, 240234710, 240692696, 241034054, 241347355, 241775141, 242206709, 242683018, 243145068, 243637892, 244008150, 244317353, 244769753, 245196224, 245710221, 246193247, 246687330, 247061943, 247410790, 247868435, 248272884, 248797676, 249315133, 249839415, 250245164, 250585863, 251110911, 251580325, 252153854, 252674737, 253274490, 253670214, 254018493, 254620079, 255126904, 255742113, 256363770, 256978407, 257416423, 257796215, 258500870, 259093463, 259751368, 260348589, 260962280, 261381085, 261770875, 262513214, 263113681, 263816145, 264523382, 265254220, 265726430, 266146231, 266849953, 267489200, 268200544, 268898928, 269592462, 270059366, 270485916, 271179799, 271816596, 272556770, 273300997, 274068563, 274598505, 275083461, 275893527, 276652188, 277566671, 278578699, 279434799, 280116791, 280692270, 282033889, 283370260, 285075127, 287025253, 288728291, 289931428, 290849939, 293188320, 295801747, 298385756, 301054978, 304008616, 306145200, 308150282, 311401997, 314432404, 317879058, 321066163, 324382616, 326810379, 328981578, 331759391, 335523995, 339607276, 343341570, 347151687, 349801473, 352184811, 355817671, 359471485, 363233158, 366932890, 370611819, 373175361, 375352380, 379211855, 382341587, 385526232, 388720309, 391675874, 393779507, 395542316, 398516432, 401116122, 403549117, 406363391, 408788228, 410513339, 411919279, 413962163, 415832953, 418072759, 420135672, 422119106, 423596187, 424789720, 426495378, 428169102, 430086590, 431874501, 433528807, 434802795, 435854949, 437463827, 439062564, 440722439, 442497646, 444260070, 445550401, 446660147, 448401620, 449985919, 451976099, 453847080, 455719680, 457280387, 458418032, 460415942, 462225344, 464464205, 466542109, 468433636, 469913015, 470873843, 472753728, 474730951, 476534854, 478319055, 480078626, 481203035, 482053796, 483898093, 485649915, 487247590, 488810861, 799, 801, 802, 803, 804, 805, 805, 807, 807, 808, 809, 811, 812, 813, 813, 815, 816, 817, 817, 819, 820, 821, 822, 823, 824, 824, 825, 826, 827, 829, 830, 831, 831, 833, 833, 834, 835, 837, 838, 839, 839, 840, 842, 843, 843, 844, 846, 847, 847, 849, 850, 850, 851, 853, 853, 854, 855, 857, 857, 859, 860, 860, 861, 862, 863, 865, 866, 867, 867, 869, 870, 870, 872, 873, 874, 874, 876, 876, 878, 879, 879, 881, 882, 882, 883, 884, 885, 887, 887, 888, 890, 890, 892, 893, 893, 894, 896, 897, 898, 898, 900, 900, 902, 902, 904, 905, 905, 907, 908, 909, 910, 910, 912, 912, 914, 915, 916, 916, 917, 919, 920, 921, 921, 922, 924, 924, 926, 927, 927, 929, 930, 931, 931, 933, 933, 934, 936, 937, 938, 938, 939, 941, 941, 943, 944, 944, 945, 946, 948, 949, 949, 951, 952, 952, 954, 955, 956, 957, 957, 958, 960, 961, 961, 963, 963, 964, 965, 966, 968, 969, 970, 971, 972, 973, 974, 975, 976, 976, 977, 979, 979, 981, 982, 983, 983, 984, 985, 987, 987, 989, 990, 991, 992, 992, 994, 995, 996, 996, 997, 998, 1000, 1000, 1002, 1003, 1003, 1005, 1005, 1006, 1007, 1009, 1009, 1010, 1011, 1012, 1014, 1015, 1016, 1017, 1017, 1018, 1020, 1020, 1022, 1022, 1023, 1024, 1025, 1027, 1028, 1028, 1030, 1030, 1032, 1032, 1033, 1035, 1035, 1036, 1038, 1038, 1039, 1040, 1041, 1043, 1044, 1044, 1046, 1046, 1048, 1048, 1049, 1051, 1051, 1052, 1054, 1054, 1055, 1056, 1058, 1058, 1059, 1060, 1061, 1062, 1064, 1065, 1065, 1066, 1068, 1069, 1070, 1071, 1071, 1072, 1073, 1075, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1082, 1083, 1084, 1086, 1087, 1087, 1088, 1089, 1091, 1091, 1092, 1094, 1095, 1095, 1096, 1098, 1099, 1099, 1101, 1102, 1102, 1103, 1104, 1105, 1107, 1107, 1108, 1110, 1111, 1111, 1113, 1113, 1115, 1115, 1116, 1118, 1118, 1119, 1121, 1122, 1122, 1123, 1125, 1126, 1127, 1127, 1128, 1129, 1131, 1131, 1133, 1134, 1135, 1135, 1137, 1137, 1139, 1139, 1141, 1141]\n"
     ]
    }
   ],
   "source": [
    "print(foreCast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(foreCast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Lists must have the same length\")\n",
    "    \n",
    "    squared_errors = [(x - y) ** 2 for x, y in zip(list1, list2)]\n",
    "    mse = sum(squared_errors) / len(list1)\n",
    "    \n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_values, predicted_values, n):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy between two lists of true values and predicted values.\n",
    "\n",
    "    Args:\n",
    "        true_values (list): List of true values.\n",
    "        predicted_values (list): List of predicted values.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy score between 0 and 1.\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(true_values)-800\n",
    "\n",
    "    for i in range(800, len(true_values)):\n",
    "        if true_values[i] == predicted_values[i]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0911481760046648e+17\n"
     ]
    }
   ],
   "source": [
    "print(calculate_mse(cases_list, foreCast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Interval' from partially initialized module 'sklearn.utils._param_validation' (most likely due to a circular import) (/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m      3\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(cases_list, foreCast)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.metrics` module includes score functions, performance metrics\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mand pairwise metrics and distance computations.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     accuracy_score,\n\u001b[1;32m     10\u001b[0m     balanced_accuracy_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     zero_one_loss,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/cluster/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.metrics.cluster` submodule contains evaluation metrics for\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mcluster analysis results. There are two forms of evaluation:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m- unsupervised, which does not and measures the 'quality' of the model itself.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     adjusted_mutual_info_score,\n\u001b[1;32m     11\u001b[0m     adjusted_rand_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     v_measure_score,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     calinski_harabasz_score,\n\u001b[1;32m     27\u001b[0m     davies_bouldin_score,\n\u001b[1;32m     28\u001b[0m     silhouette_samples,\n\u001b[1;32m     29\u001b[0m     silhouette_score,\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/cluster/_bicluster.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOptions, validate_params\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_consistent_length\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsensus_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isfinite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/__init__.py:87\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     84\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     90\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Interval, validate_params\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Interval' from partially initialized module 'sklearn.utils._param_validation' (most likely due to a circular import) (/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(cases_list, foreCast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
