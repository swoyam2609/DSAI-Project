{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 122,
=======
   "execution_count": 32,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import pandas as pd\n",
    "from array import array"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 62,
=======
   "execution_count": 33,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
=======
   "execution_count": 34,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing independent and dependent features\n",
    "def prepare_data(timeseries_data, n_features):\n",
    "\tX, y =[],[]\n",
    "\tfor i in range(len(timeseries_data)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_features\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(timeseries_data)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = timeseries_data[i:end_ix], timeseries_data[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 64,
=======
   "execution_count": 35,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_data_global= pd.read_csv(\"../Dataset/global_cases.csv\")\n",
    "timeseries_data_india= pd.read_csv(\"../Dataset/india_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 65,
=======
   "execution_count": 36,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22/01/20</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23/01/20</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24/01/20</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25/01/20</td>\n",
       "      <td>1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26/01/20</td>\n",
       "      <td>2120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  Cases\n",
       "0  22/01/20    557\n",
       "1  23/01/20    657\n",
       "2  24/01/20    944\n",
       "3  25/01/20   1437\n",
       "4  26/01/20   2120"
      ]
     },
<<<<<<< HEAD
     "execution_count": 65,
=======
     "execution_count": 36,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries_data_global.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 66,
=======
   "execution_count": 37,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22/01/20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23/01/20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24/01/20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25/01/20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26/01/20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  Cases\n",
       "0  22/01/20      0\n",
       "1  23/01/20      0\n",
       "2  24/01/20      0\n",
       "3  25/01/20      0\n",
       "4  26/01/20      0"
      ]
     },
<<<<<<< HEAD
     "execution_count": 66,
=======
     "execution_count": 37,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries_data_india.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 38,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cases=timeseries_data_global['Cases']\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 68,
=======
   "execution_count": 39,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cases_array=np.array(global_cases)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 88,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "1138    False\n",
       "1139    False\n",
       "1140    False\n",
       "1141    False\n",
       "1142    False\n",
       "Name: Cases, Length: 1143, dtype: bool"
      ]
     },
     "execution_count": 88,
=======
       "array([      557,       657,       944, ..., 676213378, 676392824,\n",
       "       676570149])"
      ]
     },
     "execution_count": 40,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_cases.isnull()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 91,
=======
   "execution_count": 41,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_cases.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_cases=timeseries_data_india['Cases']\n",
    "india_cases.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
=======
   "execution_count": 42,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [],
   "source": [
    "india_cases_array=np.array(india_cases)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 72,
=======
   "execution_count": 43,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       0,        0,        0, ..., 44689919, 44690298, 44690738])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 72,
=======
     "execution_count": 43,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_cases_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 111,
=======
   "execution_count": 44,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps=143\n",
    "X_prevdata_global,y_preddata_global=prepare_data(global_cases_array, timestamps)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 101,
=======
   "execution_count": 45,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[      557       657       944      1437      2120      2929]\n",
      " [      657       944      1437      2120      2929      5580]\n",
      " [      944      1437      2120      2929      5580      6169]\n",
      " ...\n",
      " [675542852 675731911 675914580 675968775 676024901 676082941]\n",
      " [675731911 675914580 675968775 676024901 676082941 676213378]\n",
      " [675914580 675968775 676024901 676082941 676213378 676392824]]\n"
     ]
    }
   ],
   "source": [
    "print(X_prevdata_global)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 75,
=======
   "execution_count": 46,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7793175   7925122   8050034   8193933   8337135   8480229   8661225\n",
      "   8847108   8940170   9082502   9250625   9422061   9601576   9794066\n",
      "   9970478  10138935  10291617  10475388  10690516  10895486  11098124\n",
      "  11287838  11474058  11642331  11851900  12067186  12291782  12524663\n",
      "  12737483  12930415  13124893  13345920  13575706  13820618  14058038\n",
      "  14293379  14505270  14713414  14956435  15231791  15511422  15794928\n",
      "  16044485  16256294  16490680  16754567  17024801  17306582  17594044\n",
      "  17840315  18070926  18276983  18542287  18817443  19104145  19386579\n",
      "  19651806  19884801  20118013  20383576  20658646  20948740  21255813\n",
      "  21504839  21714880  21927762  22187166  22464640  22736119  22998072\n",
      "  23261601  23464360  23692460  23937668  24219359  24506622  24790561\n",
      "  25051295  25271033  25535340  25802224  26084114  26369544  26671276\n",
      "  26944997  27173995  27391278  27637790  27923100  28227493  28543855\n",
      "  28832580  29081415  29348555  29629107  29934059  30249632  30576278\n",
      "  30869397  31124526  31384435  31668754  31980407  32301360  32631974\n",
      "  32918085  33165079  33426328  33709128  34033675  34352787  34682292\n",
      "  34979647  35240179  35551524  35867647  36216841  36576162  36936565\n",
      "  37292089  37581583  37871608  38190980  38571764  38978760  39389280\n",
      "  39761413  40092656  40467606  40856480  41296091  41778168  42272069\n",
      "  42728414  43087247  43570291  44038754  44560523  45111898  45683640\n",
      "  46155745  46594934  47143303  47708817  48200961  48820295  49443478\n",
      "  50042669  50533518  51043324  51614069  52238734  52878929  53539863\n",
      "  54126889  54617117  55142751  55745063  56367328  57021314  57695345\n",
      "  58287916  58785996  59320006  59910792  60527871  61117132  61812474\n",
      "  62420041  62915662  63422239  64037142  64677389  65367986  66064345\n",
      "  66706697  67250638  67785484  68416150  69081500  70576907  71282658\n",
      "  71931348  72470142  73021298  73669919  74387461  75126063  75847501\n",
      "  76483006  77030307  77576529  78227986  78910081  79603472  80096696\n",
      "  80629340  81042534  81541719  82213594  82938894  83778622  84342314\n",
      "  84946690  85467020  86024461  86765351  87556264  88426222  89261637\n",
      "  90017904  90619443  91233917  91926453  92669726  93428784  94212637\n",
      "  94869509  95397119  95914366  96507148  97194354  97863110  98526596\n",
      "  99103312  99565294 100046846 100608158 101209122 101824237 102413219\n",
      " 102928083 103324692 103767694 104233148 104758397 105230171 105766676\n",
      " 106201874 106553422 106893339 107300629 107741093 108184860 108614449\n",
      " 108994937 109303582 109584404 109944197 110330576 110741376 111151038\n",
      " 111526248 111845811 112141380 112534122 112979052 113432016 113873956\n",
      " 114270693 114578478 114883979 115197242 115639088 116097496 116546593\n",
      " 116960620 117335064 117640533 118054577 118513444 119003124 119492372\n",
      " 119947968 120313043 120667127 121134161 121679877 122227806 122790345\n",
      " 123288792 123729261 124150452 124656291 125285990 125938183 126578225\n",
      " 127162758 127655412 128114688 128676894 129350790 130060609 130705394\n",
      " 131245744 131805666 132302655 132910149 133573288 134409344 135156662\n",
      " 135827306 136536918 137150764 137914588 138725902 139547133 140406983\n",
      " 141200970 141901080 142592012 143431609 144320851 145220252 146127911\n",
      " 146952141 147680627 148369677 149208799 150107590 151001625 151892994\n",
      " 152687904 153375175 154065632 154850588 155692589 156565435 157400261\n",
      " 158188559 158837968 159463612 160189411 160956898 161691597 162401162\n",
      " 163032774 163579396 164122938 164742576 165414621 165696927 166323402\n",
      " 166895763 167377386 167828464 168361871 168930299 169477196 169982707\n",
      " 170463127 170853074 171240130 171700134 172187762 172668261 173093152\n",
      " 173489803 173812784 174134551 174504668 174922960 175376464 175798501\n",
      " 176166881 176469682 176778984 177153741 177542916 177938911 178342418\n",
      " 178689716 178994480 179291135 179701349 180097972 180507231 180924899\n",
      " 181287796 181599032 181932050 182313258 182712726 183148268 183590610\n",
      " 183964437 184292181 184670098 185120629 185585871 186067750 186580986\n",
      " 187002099 187371991 187806155 188331814 188869796 189446223 190042161\n",
      " 190531011 190951220 191440687 191973106 192529214 193094403 193822543\n",
      " 194254462 194700076 195240703 195847033 196497116 197139985 197872339\n",
      " 198389684 198872752 199443855 200078130 200758588 201444210 202263736\n",
      " 202825005 203292722 203911805 204557617 205288310 205994052 206795847\n",
      " 207348758 207815449 208479793 209162480 209898123 210609857 211395070\n",
      " 211954124 212402204 213086398 213763171 214500582 215237031 215988154\n",
      " 216545334 216992164 217664015 218284677 219003757 219680532 220404105\n",
      " 220900816 221347788 221825561 222482051 223119762 223759907 224418618\n",
      " 224887140 225258543 225836445 226386274 226953125 227540479 228163698\n",
      " 228687569 229050966 229581157 230069837 230604815 231152445 231701691\n",
      " 232075693 232431512 232914281 233365258 233856235 234346132 234868134\n",
      " 235218776 235527819 235974883 236397732 236904785 237366619 237842039\n",
      " 238182243 238498696 238886684 239324892 239781314 240234710 240692696\n",
      " 241034054 241347355 241775141 242206709 242683018 243145068 243637892\n",
      " 244008150 244317353 244769753 245196224 245710221 246193247 246687330\n",
      " 247061943 247410790 247868435 248272884 248797676 249315133 249839415\n",
      " 250245164 250585863 251110911 251580325 252153854 252674737 253274490\n",
      " 253670214 254018493 254620079 255126904 255742113 256363770 256978407\n",
      " 257416423 257796215 258500870 259093463 259751368 260348589 260962280\n",
      " 261381085 261770875 262513214 263113681 263816145 264523382 265254220\n",
      " 265726430 266146231 266849953 267489200 268200544 268898928 269592462\n",
      " 270059366 270485916 271179799 271816596 272556770 273300997 274068563\n",
      " 274598505 275083461 275893527 276652188 277566671 278578699 279434799\n",
      " 280116791 280692270 282033889 283370260 285075127 287025253 288728291\n",
      " 289931428 290849939 293188320 295801747 298385756 301054978 304008616\n",
      " 306145200 308150282 311401997 314432404 317879058 321066163 324382616\n",
      " 326810379 328981578 331759391 335523995 339607276 343341570 347151687\n",
      " 349801473 352184811 355817671 359471485 363233158 366932890 370611819\n",
      " 373175361 375352380 379211855 382341587 385526232 388720309 391675874\n",
      " 393779507 395542316 398516432 401116122 403549117 406363391 408788228\n",
      " 410513339 411919279 413962163 415832953 418072759 420135672 422119106\n",
      " 423596187 424789720 426495378 428169102 430086590 431874501 433528807\n",
      " 434802795 435854949 437463827 439062564 440722439 442497646 444260070\n",
      " 445550401 446660147 448401620 449985919 451976099 453847080 455719680\n",
      " 457280387 458418032 460415942 462225344 464464205 466542109 468433636\n",
      " 469913015 470873843 472753728 474730951 476534854 478319055 480078626\n",
      " 481203035 482053796 483898093 485649915 487247590 488810861 490229883\n",
      " 491102544 491795689 493029039 494394436 495614407 496892754 498067151\n",
      " 498809406 499371019 500357346 501476932 502551958 503514561 504173546\n",
      " 504728004 505135766 505593027 506611148 507572864 508494564 509326073\n",
      " 509804848 510170201 510807207 511586152 512270545 513029820 513674263\n",
      " 514028003 514337424 514784854 515456909 516084404 516684038 517314925\n",
      " 517654258 517933111 518623538 519264799 519959508 520612153 521185114\n",
      " 521623840 521997492 522587922 523196288 523956899 524552967 525179079\n",
      " 525546242 525862056 526355204 526949796 527593755 528096493 528666149\n",
      " 528995178 529271873 529616224 530241569 530939976 531470716 532014270\n",
      " 532267351 532530338 532965247 533576907 534283486 534848531 535405294\n",
      " 535689085 535887333 536415150 537272203 537994296 538577138 539110151\n",
      " 539338742 539607241 540362445 541178509 541984328 542767959 543545293\n",
      " 543862216 544148980 545021364 545963319 546896134 547974317 548925491\n",
      " 549284928 549643412 550495502 551700794 552926114 553979237 554985885\n",
      " 555489725 555912593 556965871 558188894 559615969 560618337 561620875\n",
      " 562173878 562633320 563831363 565376142 566978238 568132310 569227609\n",
      " 569864429 570415520 571389219 572585879 574012504 575128637 576504042\n",
      " 577080293 577625050 578498268 579688847 580844327 582298507 583199840\n",
      " 584378161 584876085 585692322 586732558 587759595 588765069 589518048\n",
      " 590154612 590615366 591470649 592553956 593544473 594489234 595290702\n",
      " 595865130 596349664 597060819 597949972 598834543 599699548 600401903\n",
      " 600854582 601260827 601837965 602585861 603339040 604018804 604676287\n",
      " 605052969 605423358 605881148 606554997 607171546 607755506 608281134\n",
      " 608594366 608908659 609393741 609942218 610640243 611251541 611707437\n",
      " 612002534 612257111 612673963 613227572 613745750 614365390 614883451\n",
      " 615148616 615371539 615794219 616393758 616943716 617587849 618060372\n",
      " 618301429 618502138 618822004 619422193 620005197 620645800 621221027\n",
      " 621440044 621658996 622112629 622802836 623353219 624016113 624494008\n",
      " 624712097 624904267 625380762 626009472 626536431 627131819 627563033\n",
      " 627745374 627906174 628292158 628887107 629354131 629825004 630188852\n",
      " 630387961 630555274 630855587 631257223 631605553 632099160 632543650\n",
      " 632744972 632931615 633236212 633687028 634121053 634581453 635069827\n",
      " 635312334 635495843 635842682 636346431 636832747 637312096 637942820\n",
      " 638190175 638412606 638754188 639328415 639919189 640339580 641037771\n",
      " 641308397 641571858 641977647 642708342 643244123 643950364 644711955\n",
      " 645011864 645269011 645774500 646399010 647032502 647841604 648512056\n",
      " 648786733 649085967 649658720 650436793 651122541 651873247 652605304\n",
      " 652905777 653189501 653775912 654484792 655205564 655963530 656604975\n",
      " 656949354 657135648 657655053 658309208 658946340 659617625 660199728\n",
      " 660463792 660778387 661198108 661660893 662231345 662966595 663685449\n",
      " 664042494 664369944 664718936 665113877 665620426 666097539 666533383\n",
      " 666754735 666951599 667166217 667518310 667896028 668220526 668562246\n",
      " 668715650 668830077 669001131 669272951 669567171 669848920 670086844\n",
      " 670220207 670344054 670503932 670734589 671032404 671291489 671529548\n",
      " 671632287 671721476 671867463 672065240 672295338 672554241 672696324\n",
      " 672828531 672906177 673044131 673237731 673477639 673685532 673878833\n",
      " 673969796 674056229 674143589 674323721 674569824 674790916 674933342\n",
      " 674978793 675044414 675171439 675322238 675542852 675731911 675914580\n",
      " 675968775 676024901 676082941 676213378 676392824 676570149]\n"
     ]
    }
   ],
   "source": [
    "print(y_preddata_global)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 102,
=======
   "execution_count": 47,
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m X_prevdata_global\u001b[38;5;241m=\u001b[39mX_prevdata_global\u001b[38;5;241m.\u001b[39mreshape((X_prevdata_global\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[43mX_prevdata_global\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m,n_feature))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "n_feature=1\n",
    "X_prevdata_global=np.array(X_prevdata_global)\n",
    "X_prevdata_global=X_prevdata_global.reshape((X_prevdata_global.shape[0],X_prevdata_global.shape[1],n_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "(1137, 6, 1)"
=======
       "(1133, 10)"
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prevdata_global.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
=======
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swoyamsiddharth/Library/Python/3.9/lib/python/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_5\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m65\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(timestamps, n_feature)))\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m65\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m65\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/models/sequential.py:116\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer, rebuild)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/models/sequential.py:135\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    134\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:222\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(original_build_method)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[0;32m--> 222\u001b[0m         \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/models/sequential.py:176\u001b[0m, in \u001b[0;36mSequential.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/layers/input_spec.py:186\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mallow_last_axis_squeeze:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m>\u001b[39m spec\u001b[38;5;241m.\u001b[39mmax_ndim:\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm_5\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 65)"
>>>>>>> 96482ac8071f7ceaaf4af935250c7c7efeb99bd1
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - loss: 82155964093956096.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 90316865812824064.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 90540616429076480.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 94253718735683584.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 88728492417482752.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 94231204517117952.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 86074546225938432.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 88102183106510848.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 85624708531224576.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 88851861058093056.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 87057355002347520.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 86651841370128384.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 85238075575238656.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 85519636451295232.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 91773813208907776.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 93634487530815488.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 90171867716911104.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 82488858419134464.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 85149375910641664.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 88641837157318656.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 87006236301590528.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 86885272842665984.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 81692837770428416.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 88247885577060352.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 88845315527933952.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 93485855892570112.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 84963446776397824.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 88364347910258688.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 87194295739613184.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 89667578426818560.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 84588092404531200.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 87398727592968192.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 94097828602707968.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 85234691141009408.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 85067075747315712.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 87137061005426688.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 88441339494006784.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 93916031227002880.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 91123503620685824.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 90166069511061504.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 93218262250160128.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 86765511974584320.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 85186896744939520.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 86397201349083136.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 89592854585802752.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 90362839142760448.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 85419804231467008.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 88555671523426304.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 87297916120596480.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 97120214268772352.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 83808512890634240.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 88804307180191744.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 84056530072109056.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 87605684887093248.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 89255897221562368.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 89323457057128448.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 88927246324072448.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 88304012209684480.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 91752948257783808.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 90432512102236160.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 92360668950298624.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 80590061967507456.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 82553033820471296.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 85441442276704256.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 88522711944396800.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 82049526214426624.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 89172858323861504.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - loss: 89718044292546560.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 96955064186306560.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 85450341448941568.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 86689783111221248.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 83992251591557120.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 93547325464510464.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 89167687183237120.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 89003284425080832.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 90613587923435520.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 90722259185958912.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 82217296226942976.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 91867340416745472.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - loss: 85315204597940224.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 88679701589000192.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 84346079587336192.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 87145900048121856.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 88570798398242816.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 86992440866635776.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 87268882141675520.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 92603703969710080.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 81754771198836736.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 86552911093432320.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 86366157325467648.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 100241143664476160.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 85791430571720704.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 80841283194585088.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 89535783060373504.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 84808965392695296.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 92064951862034432.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 87016415374082048.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 85704491843715072.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 93628465986666496.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - loss: 86507650728067072.0000 - val_loss: 418900151927570432.0000 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21ea2cd5950>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(timestamps, n_feature)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "# fit model\n",
    "model.fit(X_prevdata_global,y_preddata_global,batch_size=32, epochs=100,validation_split=0.2, verbose=1,callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array() argument 1 must be a unicode character, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(temp_input)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m143\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m         x_input\u001b[38;5;241m=\u001b[39m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m day input \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i,x_input))\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m#print(x_input)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: array() argument 1 must be a unicode character, not list"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction for next 10 days\n",
    "x_input = global_cases_array[:144]\n",
    "temp_input=list(x_input)\n",
    "lst_output=[]\n",
    "i=0\n",
    "while(i<10):\n",
    "    \n",
    "    if(len(temp_input)>143):\n",
    "        x_input=array(temp_input[1:])\n",
    "        print(\"{} day input {}\".format(i,x_input))\n",
    "        #print(x_input)\n",
    "        x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        #print(x_input)\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.append(yhat[0][0])\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "        lst_output.append(yhat[0][0])\n",
    "        i=i+1\n",
    "    else:\n",
    "        x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(yhat[0])\n",
    "        temp_input.append(yhat[0][0])\n",
    "        lst_output.append(yhat[0][0])\n",
    "        i=i+1\n",
    "    \n",
    "\n",
    "print(lst_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
